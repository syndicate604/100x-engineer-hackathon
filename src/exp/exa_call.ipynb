{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "CUR_DIR = os.getcwd()\n",
    "CUR_DIR = CUR_DIR.replace(\"\\\\\", \"/\").replace('/exp','')\n",
    "sys.path.append(CUR_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from app.exa import ExaAPI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "exa_api = ExaAPI(api_key=os.getenv(\"EXA_API_KEY\"))\n",
    "\n",
    "results = exa_api.search(\"artificial intelligence news\", type=\"neural\", num_results=5)\n",
    "\n",
    "contents = exa_api.get_contents(\n",
    "    [\"https://www.theverge.com/2023/11/22/23973354/a-recent-openai-breakthrough-on-the-path-to-agi-has-caused-a-stir\"]\n",
    ")\n",
    "\n",
    "# Combined search and contents\n",
    "search_contents = exa_api.search_and_contents(\n",
    "    \"latest AI developments\", type=\"neural\", num_results=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_res = [search_contents.results[i].text for i in range(len(search_contents.results))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Everything you need to know about the state of AI.            There‚Äôs no denying that the AI industry moves fast. Each week brings a bold new announcement, product release, or lofty claim that pushes the bounds of what we previously thought was possible. Separating AI fact from hyped-up fiction isn‚Äôt always easy. That‚Äôs why we‚Äôve created the AI Hype Index‚Äîa simple, at-a-glance summary of everything you need to know about the state of the industry. Our first index is a white-knuckle ride that ranges from the outright depressing‚Äîrising numbers of sexually explicit deepfakes; the complete lack of rules governing Elon Musk‚Äôs Grok AI model‚Äîto the bizarre, including AI-powered dating wingmen and startup Friend‚Äôs dorky intelligent-jewelry line.  But it‚Äôs not all a horror show‚Äîat least not entirely. AI is being used for more wholesome endeavors, too, like simulating the classic video game Doom without a traditional gaming engine. Elsewhere, AI models have gotten so good at table tennis they can now beat beginner-level human opponents. They‚Äôre also giving us essential insight into the secret names monkeys use to communicate with one another. Because while AI may be a lot of things, it‚Äôs never boring.      Deep Dive   Artificial intelligence    Stay connected       Get the latest updates fromMIT Technology Review Discover special offers, top stories,\\nupcoming events, and more.',\n",
       " \"Sponsor      Amazon is focusing on using A.I. to get stuff delivered to you faster     Hugging Face releases the first RNN in transformers!      C3.ai stock pops, company raises outlook amid 'accelerating' AI interest     Willow - A Practical, Open Source, Privacy-Focused Platform for Voice Assistants and other Applications     Qualcomm CEO: ‚ÄòA.I. is going to touch every corner of our lives. Here‚Äôs how our devices will change to make it happen‚Äô       MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers abs:  https://arxiv.org/abs/2305.07185     paper page:  https://huggingface.co/papers/2305.07185  abstract: Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.             ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4   abs:  https://arxiv.org/abs/2305.07490     paper page:  https://huggingface.co/papers/2305.07490   model:  https://huggingface.co/Tyrannosaurus/ArtGPT-4  abstract: In recent years, large language models (LLMs) have made significant progress in natural language processing (NLP), with models like ChatGPT and GPT-4 achieving impressive capabilities in various linguistic tasks. However, training models on such a large scale is challenging, and finding datasets that match the model's scale is often difficult. Fine-tuning and training models with fewer parameters using novel methods have emerged as promising approaches to overcome these challenges. One such model is MiniGPT-4, which achieves comparable vision-language understanding to GPT-4 by leveraging novel pre-training models and innovative training strategies. However, the model still faces some challenges in image understanding, particularly in artistic pictures. A novel multimodal model called ArtGPT-4 has been proposed to address these limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100 device in just 2 hours, using only about 200 GB of data. The model can depict images with an artistic flair and generate visual code, including aesthetically pleasing HTML/CSS web pages. Furthermore, the article proposes novel benchmarks for evaluating the performance of vision-language models. In the subsequent evaluation methods, ArtGPT-4 scored more than 1 point higher than the current state-of-the-art model and was only 0.25 points lower than artists on a 6-point scale.             Optimizing Memory Mapping Using Deep Reinforcement Learning   abs:  https://arxiv.org/abs/2305.07440     paper page:  https://huggingface.co/papers/2305.07440  abstract: Resource scheduling and allocation is a critical component of many high impact systems ranging from congestion control to cloud computing. Finding more optimal solutions to these problems often has significant impact on resource and time savings, reducing device wear-and-tear, and even potentially improving carbon emissions. In this paper, we focus on a specific instance of a scheduling problem, namely the memory mapping problem that occurs during compilation of machine learning programs: That is, mapping tensors to different memory layers to optimize execution time. We introduce an approach for solving the memory mapping problem using Reinforcement Learning. RL is a solution paradigm well-suited for sequential decision making problems that are amenable to planning, and combinatorial search spaces with high-dimensional data inputs. We formulate the problem as a single-player game, which we call the mallocGame, such that high-reward trajectories of the game correspond to efficient memory mappings on the target hardware. We also introduce a Reinforcement Learning agent, mallocMuZero, and show that it is capable of playing this game to discover new and improved memory mapping solutions that lead to faster execution times on real ML workloads on ML accelerators. We compare the performance of mallocMuZero to the default solver used by the Accelerated Linear Algebra (XLA) compiler on a benchmark of realistic ML workloads. In addition, we show that mallocMuZero is capable of improving the execution time of the recently published AlphaTensor matrix multiplication model.             Universal Source Separation with Weakly Labelled Data   abs:  https://arxiv.org/abs/2305.07447     paper page:  https://huggingface.co/papers/2305.07447   github:  https://github.com/bytedance/uss  abstract: Universal source separation (USS) is a fundamental research task for computational auditory scene analysis, which aims to separate mono recordings into individual source tracks. There are three potential challenges awaiting the solution to the audio source separation task. First, previous audio source separation systems mainly focus on separating one or a limited number of specific sources. There is a lack of research on building a unified system that can separate arbitrary sources via a single model. Second, most previous systems require clean source data to train a separator, while clean source data are scarce. Third, there is a lack of USS system that can automatically detect and separate active sound classes in a hierarchical level. To use large-scale weakly labeled/unlabeled audio data for audio source separation, we propose a universal audio source separation framework containing: 1) an audio tagging model trained on weakly labeled data as a query net; and 2) a conditional source separation model that takes query net outputs as conditions to separate arbitrary sound sources. We investigate various query nets, source separation models, and training strategies and propose a hierarchical USS strategy to automatically detect and separate sound classes from the AudioSet ontology. By solely leveraging the weakly labelled AudioSet, our USS system is successful in separating a wide variety of sound classes, including sound event separation, music source separation, and speech enhancement. The USS system achieves an average signal-to-distortion ratio improvement (SDRi) of 5.57 dB over 527 sound classes of AudioSet; 10.57 dB on the DCASE 2018 Task 2 dataset; 8.12 dB on the MUSDB18 dataset; an SDRi of 7.28 dB on the Slakh2100 dataset; and an SSNR of 9.00 dB on the voicebank-demand dataset.             Better speech synthesis through scaling   abs:  https://arxiv.org/abs/2305.07243     paper page:  https://huggingface.co/papers/2305.07243  abstract: In recent years, the field of image generation has been revolutionized by the application of autoregressive transformers and DDPMs. These approaches model the process of image generation as a step-wise probabilistic processes and leverage large amounts of compute and data to learn the image distribution. This methodology of improving performance need not be confined to images. This paper describes a way to apply advances in the image generative domain to speech synthesis. The result is TorToise -- an expressive, multi-voice text-to-speech system.            Thank you for reading AK‚Äôs Substack. This post is public so feel free to share it.   Share\",\n",
       " 'Welcome to the AI News Briefs Bulletin Board, a timely new channel bringing you the latest industry insights and perspectives surrounding the field of AI including deep learning, large language models, generative AI, and transformers. I am working tirelessly to dig up the most timely and curious tidbits underlying the day‚Äôs most popular technologies. I know this field is advancing rapidly and I want to bring you a regular resource to keep you informed and state-of-the-art. The news bites are constantly being added in reverse date order (most recent on top). With the bulletin board you can check back often to see what‚Äôs happening in our rapidly accelerating industry. Click HERE to check out previous ‚ÄúAI News Briefs‚Äù round-ups.\\n    \\n    \\n[9/17/2024] Google Launched NotebookLM with AI-Powered Audio Podcast and Summaries ‚Äì NotebookLM is Google Labs‚Äô new tool that converts uploaded documents into AI-generated audio discussions in podcast format. It transforms text from PDFs, Google Docs, and Slides into podcast-style conversations between two AI hosts. This approach aims to enhance information retention through auditory learning.\\nThe system uses Gemini 1.5‚Äôs multimodal capabilities to process various input formats. It analyzes content, extracts key information, and generates responses grounded in the source material. The system employs advanced text-to-speech synthesis for AI host voices. It generates contextually relevant dialogue, maintaining coherence with source material. However, occasional inaccuracies may occur, and user interaction during playback is not yet supported. NotebookLM provides citations and relevant quotes to support its outputs. Audio Overview‚Äôs core innovation lies in its AI-driven dialogue synthesis. The system creates a conversational summary of uploaded content, with AI agents discussing main points and connecting topics. \\nReported technical specifications:\\nContext window capacity: ~50 PDFs\\nLanguage support: Currently English-only\\nInput formats: PDFs, Google Docs, Slides, web URLs\\nOutput: Downloadable audio file\\nNotebookLM‚Äôs architecture manages large input volumes and cross-references information between multiple documents. \\nHowever, audio generation for extensive notebooks can take several minutes, indicating computational intensity.\\n    \\n[9/17/2024] NEW research paper: ‚ÄúLLMs Will Always Hallucinate, and We Need to Live With This.‚Äù ‚Äì LLMs inevitably hallucinate due to fundamental mathematical and logical limitations. The paper proves hallucinations are structural and cannot be eliminated through architectural improvements, dataset enhancements, or fact-checking mechanisms.\\nThe authors introduce the concept of ‚ÄúStructural Hallucinations‚Äù and provide mathematical proofs using computational theory and G√∂del‚Äôs First Incompleteness Theorem. They demonstrate undecidability in key LLM processes: training data completeness, information retrieval, intent classification, output generation, and fact-checking.\\nThe paper proves that every stage of LLM processing has a non-zero probability of producing hallucinations. It illustrates these concepts using popular LLMs (OpenAI, Gemini, Claude) on a specific prompt, showing deviation from expected responses.\\n    \\n[9/17/2024] NEW research paper: ‚ÄúWhat is the Role of Small Models in the LLM Era: A Survey,‚Äù ‚Äì This paper explores the role of small models in the era of LLMs. It addresses the challenges of high computational costs and energy consumption associated with large models, which limit their accessibility for many researchers and businesses. The paper investigates how small models can complement or compete with LLMs in various scenarios.\\nThe authors systematically examine the relationship between LLMs and small models from two perspectives: collaboration and competition. They analyze how small models can enhance LLMs in tasks like data curation, efficient inference, and deficiency repair. Conversely, they explore how LLMs can improve small models through knowledge distillation and data synthesis.\\nThe research reveals that small models remain highly popular, with BERT-base being one of the most downloaded models. It identifies scenarios where small models are preferable, such as computation-constrained environments, task-specific applications, and situations requiring interpretability. The paper provides valuable insights for practitioners on optimizing resource usage and developing cost-effective systems.\\n    \\n    \\n[9/17/2024] CalypsoAI just launched a new, cybercrime-solving game called ‚ÄúBehind the Mask.‚Äù The desktop game showcases the power of CalypsoAI‚Äôs GenAI scanners and gives consumers the unique opportunity to interact with CalypsoAI‚Äôs technology and GenAI in a fun and compelling way.\\nThe premise of the game is that YOU are the investigator, trying to identify all the five hackers stealing valuable cryptocurrency‚ÄîCalyptoCoins. Good news: you have the help of an AI informant. You must ask it the right questions to identify and stop the theft before more CalyptoCoins are lost. But there‚Äôs a twist‚Ä¶ The hackers are using CalypsoAI‚Äôs technology to protect that information, and the GenAI scanners learn from each ‚Äúleak‚Äù and improve defenses‚Äîmaking each level harder than the last. So, can you outsmart AI?\\n    \\n[9/16/2024] AI is accelerating the climate crisis ‚Äì If you care about the environment, think twice about using AI. Generative artificial intelligence uses 30 times more energy than a traditional search engine, warns researcher Sasha Luccioni, on a mission to raise awareness about the environmental impact of the hot new technology.\\n    \\n    \\n[9/16/2024] At its Dreamforce conference in San Francisco this week, Salesforce Ventures announced an additional $500 million fund dedicated to AI companies, bringing its total AI investment fund to $1 billion. This marks a significant increase, as Salesforce Ventures doubled its AI fund to $500 million in June 2023.\\nSince its launch in 2009, Salesforce Ventures has deployed $5 billion, and its growing AI portfolio includes notable companies like Anthropic, Hugging Face, Runway, and Together AI. San Francisco‚Äôs AI boom is attracting startups from around the world, with Salesforce Ventures playing a key role in fostering the city‚Äôs AI ecosystem.\\n    \\n[9/16/2024] Jeff Dean, on the long term potential of multi-modal models like Gemini ‚Äì Learn about Google‚Äôs AI journey from Jeff Dean. Explore the evolution of neural networks, Google Brain, and DeepMind. Discover insights on large-scale model training, reinforcement learning, and multimodal AI. Understand the development of Transformers and Gemini. Gain knowledge on cutting-edge AI techniques and their practical applications.\\n \\n \\n \\n    \\n[9/16/2024] Tweet by Astrophysics Ph.D. goes viral demoing Open AI Strawberry model replicated 10 months of Ph.D. coding work in ~ 6 prompts using paper‚Äôs methods section ‚Äì Dr. Kyle Kabasares found that after about 6 prompts, ChatGPT o1‚Äôs preview and mini created a running version of the code described from the methods section of his research paper. He emphasizes that while the skeletal code does emulate what his actual code does, it did use its own synthetic data that he asked for it to create as opposed to real astronomical data that would be used in a real paper. Nevertheless, the potential it has is incredible to effectively accomplish what he struggled for about 10 months in the first year of his Ph.D. at UC Irvine (measuring black hole masses by modeling astronomical data).',\n",
       " 'Photo by Derek Lee\\n \\nAndriy Burkov\\n  \\nAndriy Burkov\\nML at TalentNeuron, author of üìñ The Hundred-Page Machine Learning Book and üìñ the Machine Learning Engineering book\\nPublished Nov 23, 2022\\nHey, in this issue: why Meta‚Äôs latest large language model survived only three days online; Yoshua Bengio on the past, present, and future of deep learning; an RNN with transformer-level LLM performance; acquisition of chess knowledge in AlphaZero; new records for the biggest and smallest AI computers; and more.\\nThis issue is sponsored by   Superb AI   and   NannyML  .\\nWant to improve your video annotation process?   Discover the best methods for simplifying video annotation efforts   and getting the most value from every frame for your computer vision project needs.\\nRecommended by LinkedIn\\nMore than 600,000 subscribers are reading this newsletter. If you are building an AI or a data product or service, you can become a sponsor of one of the future newsletter issues and get your business featured in the newsletter. Feel free to reach out to  hello@truepositive.ca  for more details on sponsorships.\\nEnjoy the newsletter? Help us make it bigger and better by sharing it with your colleagues and friends.\\n \\nArtificial Intelligence\\n  \\n \\nArtificial Intelligence\\n833,367 followers\\n \\nInsights from the community\\nOthers also viewed\\nExplore topics',\n",
       " 'A recent OpenAI breakthrough on the path to AGI has caused a stir. Reports from  Reuters  and  The Information  Wednesday night detail an OpenAI model called Q* (pronounced Q Star) that was recently demonstrated internally and is capable of solving simple math problems. Doing grade school math may not seem impressive, but the reports note that, according to the researchers involved, it could be a step toward creating artificial general intelligence (AGI). After the publishing of the Reuters report, which said senior exec Mira Murati told employees that a letter about Q* ‚Äúprecipitated the board‚Äôs actions‚Äù to fire Sam Altman last week, OpenAI spokesperson Lindsey Held Bolton refuted that notion in a statement shared with The Verge: ‚ÄúMira told employees what the media reports were about but she did not comment on the accuracy of the information.‚Äù Separately, a person familiar with the matter told The Verge that the board never received a letter about such a breakthrough and that the company‚Äôs research progress didn‚Äôt play a role in Altman‚Äôs sudden firing. The drama continues!',\n",
       " 'Welcome to State of AI Report 2023\\n Published by Nathan Benaich on 12 October 2023. \\n üëã Read the 2023 Report  \\nFor much of the last year, it‚Äôs felt like Large Language Models (LLMs) have been the only game in town. While the State of AI Report predicted that transformers were emerging as a general purpose system back in 2021, significant advances in capabilities caught both the AI community and wider world by surprise, with implications for research, industry dynamics, and geopolitics.\\nLast year‚Äôs State of AI report outlined the rise of decentralization in AI research, but OpenAI‚Äôs GPT-4 stunned observers as big tech returned with a vengeance. Amid the scrabble for ever more compute power, challengers have found themselves increasingly reliant on its war chest. At the same time, the open source community continues to thrive, as the number of releases continues to rocket.\\nIt has also led the drawing of new fault lines, with traditional community norms around openness under pressure from both commercial imperatives and safety fears.\\nWe‚Äôve seen technical reports on state-of-the-art LLMs published that contain no useful information for AI researchers, while some labs have simply stopped producing them at all. One of the co-founders of OpenAI went as far as describing their original open source philosophy as ‚Äúflat out ‚Ä¶ wrong‚Äù. In contrast, Meta AI has emerged as the champion of open(ish) AI, with their LLaMa model family acting as the most powerful publicly accessible alternative‚Ä¶for now.\\nThe discussion around openness is taking place against the backdrop of an impassioned debate about how we navigate governance and (existential) risk. As we forecast in last year‚Äôs report, safety has shed its status as the unloved cousin of the AI research world and took center-stage for the first time. As a result, governments and regulators around the world are beginning to sit up and take notice. This has been all the more challenging as the many of the mooted models of global governance require long-standing geopolitical rivals, currently locked in the chip wars, to cooperate. Indeed, State of AI Report co-author Ian Hogarth has been seconded to chair the UK Government‚Äôs Frontier AI Taskforce, so has therefore stepped back from writing this year.\\nHowever, this is the State of AI, not the state of LLMs, and the report dives into progress in other areas of the field - from breakthroughs in navigation and weather predictions through to self-driving cars and music generation. This has been one of the most exciting years to produce this report and we believe that it will have something for everyone - from AI research through to politics.\\n Key takeaways:\\n GPT-4 is the master of all it surveys (for now), beating every other LLM on both classic benchmarks and exams designed to evaluate humans, validating the power of proprietary architectures and reinforcement learning from human feedback.\\n Efforts are growing to try to clone or surpass proprietary performance, through smaller models, better datasets, and longer context. These could gain new urgency, amid concerns that human-generated data may only be able to sustain AI scaling trends for a few more years.\\n LLMs and diffusion models continue to drive real-world breakthroughs, especially in the life sciences, with meaningful steps forward in both molecular biology and drug discovery.\\n Compute is the new oil, with NVIDIA printing record earnings and startups wielding their GPUs as a competitive edge. As the US tightens its restrictions on trade restrictions on China and mobilizes its allies in the chip wars, NVIDIA, Intel, and AMD have started to sell export-control proof chips at scale.\\n GenAI saves the VC world, as amid a slump in tech valuations, AI startups focused on generative AI applications (including video, text, and coding), raised over $18 billion from VC and corporate investors.\\n The safety debate has exploded into the mainstream, prompting action from governments and regulators around the world. However, this flurry of activity conceals profound divisions within the AI community and a lack of concrete progress towards global governance, as governments around the world pursue conflicting approaches.\\n Challenges mount in evaluating state of the art models, as standard LLMs often struggle with robustness. Considering the stakes, as ‚Äúvibes-based‚Äù approach isn‚Äôt good enough.\\nThe report is a team effort and we‚Äôre incredibly grateful to Othmane Sebbouh, Corina Gurau, and Alex Chalmers from Air Street Capital without whom the report wouldn‚Äôt have been possible this year. Thank you to our reviewers who kept us honest and to the AI community who continue to create the breakthroughs that power this report.\\nWe write this report to compile the most interesting things we‚Äôve seen, with the aim of provoking an informed conversation about the state of AI. So, we would love to hear any thoughts on the report, your take on our predictions, or any contribution suggestions for next year‚Äôs edition.\\nEnjoy reading!\\nNathan and the Air Street Capital team\\n üè° Home\\nüìß Sign up to our newsletter  \\nAuthored on the interwebs by:\\n   This work is licensed under a Creative Commons Attribution 4.0 International License.',\n",
       " 'The Bigger Picture, December 31, 2023           Six months ago and roughly 180 daily posts on AI ago, I published my  AI Outlook for the second half of 2023.  I‚Äôd stressed that these are still very early days in the  AI Tech Wave .            Those six months of 2023 are now of course coming to a close today. In today‚Äôs ‚ÄòBigger Picture‚Äô, I‚Äôm reviewing how  the last six months  have fared vs my expectations. We‚Äôll review it again mid-year at the end of June of next year. And of course my AI outlook for the  next three years . For now, let‚Äôs get started on the near-term look at the six months past and ahead.   It doesn‚Äôt feel like just six months have passed. So  much has happened in AI terms  this year, and at such a frenetic pace. There used to be a saying in the  Internet boom  years of the nineties than an internet year was seven regular years. We need to update that for the AI years. Perhaps half a year in AI terms may end up being seven regular years. Something to think about on another day.   The key expectations for the second half of 2023 as the  AI world looked on June 28th  this year, were as follows:      ‚ÄúAI still being invented and re-invented‚Äù:   Although 2023 has seen a lot of AI innovations, like progress in multimodal AI systems from companies like  Meta ,  Google , and of course  OpenAI/Microsoft  and others, much more needs to be done, especially in the area of  AI reliability and reducing hallucinations.  I suspect that may take longer than the next six months of 2024. We are still at these systems hallucinating a quarter of the time or more. The other big area of innovation has been in  open source   ‚ÄòSmall AI‚Äô systems  that  run on local devices . Have high hopes here for the year to come, especially from  Apple .              ‚ÄúEarly days in Horse race‚Äù :   For most of 2023, it certainly looked like  OpenAI and Microsoft had the race all but sewn up . Especially after  their partnership  and launch of ChatGPT last November. And of course  the successes  since then. But they had  self-inflicted stumbles  and  dramatic recoveries  along  the way . And a lot of companies have done pretty impressive things, not the least of it being Google with Bard augmenting  Google Search, and Gemini LLM AI systems to come  early next years. I‚Äôm still calling  Google as being the near-term winner in AI Augmented Search  this year and beyond. But  this and other races continue  into 2024.               ‚ÄúChip Constraints Continue‚Äù :    GPU shortage  issues saw Nvidia, up almost 250% in 2023, deliver the best stock market performance among the  ‚ÄòMagnificent 7‚Äô  big tech companies. Collectively their leadership and  public market enthusiasm for AI stories  drove the Nasdaq up over 50% this year, better than the other market averages which also had near record performances. Chip constraints will likely continue into 2024 and potentially into 2025. But  TSMC and other chip fabs  have been ramping up enough to mitigate some of those shortages. Lot more to track here in 2024.                ‚ÄúBeware AI Veneers‚Äù :   As I mentioned in  an earlier post , most of the  AI multi-hundred billion in investment  this year has been in using current AI technologies to augment and add-on existing software and hardware technologies with new AI capabilities. What I‚Äôve called ‚ÄòAI veneers‚Äô. What I call  ‚ÄòAI Native‚Äô innovations , especially in  multimodal  and  particularly voice driven AI innovations , are a smaller fraction of the overall progress. But those are the ones that capture most of the headlines. That certainly won‚Äôt change in 2024. Watch this race.               ‚ÄúRising Regulatory Drumbeats‚Äù :   There was a lot of frenetic activity on this front, especially with  Europe and the EU  running  ahead with their EU AI Act . That will still likely take at least a couple of years or more to be streamlined and put into effect. The US saw the  White House issue a voluminous set of AI Executive Orders  this year, and the Senate and Congress have planned much more AI related deliberations into 2024. Key here is how restrictive these regulations can get ahead of the actual AI innovations that potentially may be the most risky over time. That AI ‚ÄòSpeed vs Safety‚Äô issue is the current bane of the AI industry, having already almost stopped OpenAI in its track late this year. Expect lots more drama here in 2024.              ‚ÄúGeopolitics trump globalized tech supply chains‚Äù :   And of course  the US-China efforts to ‚Äòthread the needle‚Äô  despite the national security and trade issues around technology and  geopolitics continue  into 2024. What‚Äôs been encouraging is that despite a lot of  aggressive actions on both sides driven by politics , both sides have so far kept their eye on the longer term ball of their true north longer-term interests to cooperate rather than  compete more contentiously.  For now the better  ‚ÄòCooperation‚Äô focused Prisoner‚Äôs Dilemma Game Theory  continues in practice.             As I‚Äôve said in my ‚Äòcatch our AI breath‚Äô  Interregnum piece on Christmas day,  we have a  lot more to pay attention to  in 2024. not least of it being the  massive investments  and  aggressive moves  by all the  tech companies big  and  small .            Tomorrow brings a New Year with an  accelerating amount of AI driven change . Enjoy the New Year‚Äôs Eve celebrations today, and let‚Äôs brace ourselves for 2024. Stay tuned.    (NOTE: The discussions here are for information purposes only, and not meant as investment advice at any time. Thanks for  joining us here )',\n",
       " 'top 5 trending ai research with code          AK    Apr 23, 2023        5        Share          Ask-Anything, a simple yet interesting tool for chatting about video with chatGPT, miniGPT4 and StableLM  github:  https://github.com/OpenGVLab/Ask-Anything   demo:  http://106.14.223.212:7860/              MOSS, An open-source tool-augmented conversational language model from Fudan University  github:  https://github.com/OpenLMLab/MOSS            Inpaint Anything: Segment Anything Meets Image Inpainting  github:  https://github.com/geekyutao/inpaint-anything   demo:  https://huggingface.co/spaces/InpaintAI/Inpaint-Anything            DINOv2: Learning Robust Visual Features without Supervision  github:  https://github.com/facebookresearch/dinov2               Anything-3D, present a project combining  Segment Anything  with a series of 3D models   github:  https://github.com/anything-of-anything/anything-3d             AK‚Äôs Substack is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.     Thank you for reading AK‚Äôs Substack. This post is public so feel free to share it.   Share         5        Share',\n",
       " \"The State of AI Report analyses the most interesting developments in AI. We aim to trigger an informed conversation about the state of AI and its implication for the future. The Report is produced by AI investors  Nathan Benaich  and  Ian Hogarth .\\n Download 2022 Report\\n \\n \\nCompute Index\\nüìß Newsletter  Now in its fifth year, the State of AI Report 2022 is reviewed by leading AI practioners in industry and research. It considers the following key dimensions, including a new Safety section:\\n Research: Technology breakthroughs and their capabilities. \\n Industry: Areas of commercial application for AI and its business impact. \\n Politics: Regulation of AI, its economic implications and the evolving geopolitics of AI. \\n Safety: Identifying and mitigating catastrophic risks that highly-capable future AI systems could pose to us. \\n Predictions: What we believe will happen and a performance review to keep us honest. \\n Key themes in the 2022 Report include:\\n New independent research labs are rapidly open sourcing the closed source output of major labs. Despite the dogma that AI research would be increasingly centralised among a few large players, the lowered cost of and access to compute has led to state-of-the-art research coming out of much smaller, previously unknown labs. Meanwhile, AI hardware remains strongly consolidated to NVIDIA.\\n Safety is gaining awareness among major AI research entities, with an estimated 300 safety researchers working at large AI labs, compared to under 100 in last year's report, and the increased recognition of major AI safety academics is a promising sign when it comes to AI safety becoming a mainstream discipline.\\n The China-US AI research gap has continued to widen, with Chinese institutions producing 4.5 times as many papers than American institutions since 2010, and significantly more than the US, India, UK, and Germany combined. Moreover, China is significantly leading in areas with implications for security and geopolitics, such as surveillance, autonomy, scene understanding, and object detection.\\n AI-driven scientific research continues to lead to breakthroughs, but major methodological errors like data leakage need to be interrogated further. Even though AI breakthroughs in science continue, researchers warn that methodological errors in AI can leak to these disciplines, leading to a growing reproducibility crisis in AI-based science driven in part by data leakage.\\n Read more in our blog post \\n 2021 Edition\\n2020 Edition\\n2019 Edition\\n2018 Edition\\nCompute Index  \\nCo-authored on the interwebs by:\\n   This work is licensed under a Creative Commons Attribution 4.0 International License.\",\n",
       " 'It‚Äôs barely 3 months into 2023 and it is clear that AI is going to dominate the tech space this year, and very likely this decade, by a wide margin (over Web3, Cloud, Mobile or anything else in the past couple decades). If you are in tech, you have to be living under a rock to not see that AI represents a generational shift in not just the way we will use products but also in the way all products are built and how businesses operate. As such, AI is opening new growth opportunities for all businesses, including those that I and the team &amp; open source community I work with (at akash.network), are focused on (decentralized cloud computing).\\nBefore starting to work on our AI offering (at Akash Network) I spent a fair amount of time educating myself about the AI landscape, starting around the time that ChatGPT got announced. And boy, have I had a lot of catching up to do for the last 6 months (and still struggle to keep pace with developments, each week)!\\nSo, I figured I may save you some of the time I spent reading countless articles, blog posts, press releases, trying new products and building some prototypes myself, by sharing some high level learnings.\\n Intended Audience: This is not a beginners ‚Äúwhat is AI?‚Äù post or an super advanced ‚Äúhow does ControlNet modify the base stable diffusion model?‚Äù or ‚Äúwhat are different types of tokenization methods?‚Äù type post, but somewhere in between. It is intended for people who have a general understanding of AI models and follow the ‚Äúbuzz‚Äù but are looking for a cohesive understanding of the players, use cases and high level model development process.\\nNew Generative AI Models every week\\nOn average one new Generative AI model was announced almost every week, thus far in 2023. Whether it be big announcements from OpenAI, Microsoft, Google and Meta, or less ‚Äúmainstream‚Äù models like Stable Diffusion (built through a collaboration between CompVis, StabilityAI and RunwayML) or non mainstream products and models like Midjourney, ControlNet, Palm-E or Conformer-1.\\nSome clear themes here:\\nThere are multiple competing versions of everything. Whether it be the base model (for example: DALL-E-2, Dreambooth, Stable Diffusion, and Midjourney are all competing for the ‚Äútext-to-image‚Äù market, while Microsoft, Google and Notion are all competing for the ‚Äútext analysis, suggestion, summarization‚Ä¶‚Äù market) or for services offered on top of a base model to make it easy to use (for example, both StabilityAI and RunwayML are services built on top of the same open source stable diffusion model from Compvis)\\nFor Text generation (and LLMs in general): GPT-4 from OpenAI is the undisputed king but Google‚Äôs BERT, PaLM and Meta‚Äôs LLaMA are popular open source options.\\nThe rise of AI chat applications: Led by the viral success of ChatGPT (created by OpenAI), new competitors are emerging to capitalize on the growing demand for LLM-powered chat tools. Notably, Anthropic‚Äôs Claude, a highly customizable competitor to ChatGPT and Bing Chat.\\nFor image generation: Midjourney seems to be in the lead for closed source models (even ahead of OpenAI‚Äôs DALL-E) and Stable Diffusion is the most popular open source option. The contrast between the two is almost akin to the Apple/iOS and Android alternative.\\n Huggingface has become the defacto repository for AI/ML sharing and collaboration, aka the ‚ÄúGithub for AI models and datasets.‚Äù\\n \\nRace to the bottom with ‚ÄúOpen AI API wrapped‚Äù App clones\\nOutside of a few of the big tech companies (MS, Google, Meta) who have their own Generative AI models and LLMs, most of the AI productivity applications being built today (copy.ai, jasper.ai, playground.ai and countless others) are essentially wrappers on top of Open AI‚Äôs APIs. While they are seeing a lot of buzz, it is hard to imagine this continuing forever. While some of these will go on to generate 100s of millions of dollars in revenue, eventually this will be a race to the bottom for many of these startups and they will either consolidate or get acquired or just fade away. Most of the productivity AI applications will get integrated into existing bigger ‚Äútraditional‚Äù applications (MSWord, GDocs, Slack, Teams, Salesforce, Hubspot, Notion, Adobe, Autodesk, VSCode, Replit, Github etc).\\nThe disruption to the above trend will only happen when one of the non-OpenAI models surpasses them in capabilities, efficiency, price and/ or functionality. This may happen from relatively unknown companies like Assembly.ai or from one of the big tech models (subsequent iterations/ generations of BERT, PaLM-E, LLaMA and others) or both.\\nUsers may favor non-OpenAI models and services, if they prefer greater control as well. This definitely has similarities to the Apple (closed) + Android (open) ecosystem, both thriving alongside each other, in some ways.\\nAnd to be clear, there are some very valid and lucrative cases for both text analysis (in productivity apps like MS O365, GSuite, Notion and others) as well as for image generation (in Adobe, Autodesk, Canva and others). Here is a quick run down of the use cases for AI image generation:\\n Avatar creation: Applications like Avatar AI use text-to-image AI to generate custom avatars for users based on their textual descriptions of physical features, clothing, and accessories.\\n Online shopping: Some e-commerce websites use text-to-image AI to generate product images that show how items might look in different colors, styles, or settings based on user input.\\n Gaming: Video game developers use text-to-image AI to create game assets like characters, environments, and objects based on textual descriptions or scripts.\\n Digital art: Some artists use text-to-image AI to generate digital artwork based on written prompts, poetry, or other creative writing.\\n Graphic design: Text-to-image AI can be used by graphic designers to create custom graphics, logos, or illustrations based on textual descriptions of their clients‚Äô needs.\\n Advertising: Advertisers can use text-to-image AI to create compelling visual ads based on written ad copy or product descriptions.\\n Virtual interior design: Applications that generate realistic 3D environments based on textual descriptions can be used by interior designers or homeowners to visualize and plan room layouts and decor.\\n Storytelling: Writers or game developers can use text-to-image AI to generate visuals for their stories, games, or interactive narratives based on their written scripts or prompts.\\nIf you are looking for more apps built on Open AI (and other models like Stable Diffusion) APIs, gpt3demo has a nicely sorted collection of more than 700 such apps, that seems fairly up to date.\\n \\nGPUs are the new oil (for tech)\\nGPUs from Nvidia have been in high-demand for several months now, with demand showing no signs of slowing. The challenge is sourcing GPUs (like A100 and H100) that can allow these applications to run at a level of performance that doesn‚Äôt hurt user experience. This favors the big players who have either amassed a large inventory of GPUs or have the volume/ demand leverage to acquire them or are building their own tensor optimized hardware. It also puts Nvidia in a position of very strong leverage which it continues to try to cement by building it‚Äôs own cloud/ service layer (Nvidia DGX Cloud) that sits between the big clouds and Nvidia‚Äôs GPUs.\\nThere are 4 things that will upset this equation:\\n Alternatives to Nvidia Chips: AMD, Intel &amp; Xilinx are eager to take marketshare from Nvidia, after having underestimated or not planned for the potential of Generative AI.\\n Big cloud leverage: The big cloud players recognize the vulnerable position they‚Äôre in, with respect to Nvidia (who can dictate prices) and are making their custom silicon to reduce dependency (like Google TPUs and Amazon Graviton/ Inferentia/ Tranium). In addition, expect to see more partnerships between the big clouds and players in 1 above.\\n Software Optimizations: Meanwhile, software optimizations that let models run on lighter hardware, continue to emerge. Best example of this is Neuralmagic‚Äôs Deepsparse technology (I‚Äôve verified running BERT using this on a 2 Core vCPU and it works great) and ports of facebook‚Äôs LLaMA, like alpaca, that can be run on an M1 Mac or even a Raspberry Pi.\\n Distributed GPU compute solutions: These have the potential to offer an alternative where a bunch of lower end GPUs (and GPU clusters) can be used in the absence of one higher end one. Some examples of this are companies working on GPU-over-IP type solutions. While these may not be applicable to some of the generative AI applications today, there are definitely use cases for them.\\nThe key thing to keep in mind is that the GPU market was already predicted to explode by 10x, even before these new generative AI models were invented. Here is a quick post I wrote 6 months ago, about what an ‚ÄúEMBARRASSINGLY PARALLEL‚Äù workload is and how it is the driving force behind the explosive growth of GPU usage, over the past decade.\\n \\nOpen Source is alive and well in AI/ML\\nWhile the buzz around OpenAI (which to be clear is NOT open source, even though it originally started out that way) and its announcements overshadow everything else, it is becoming clear that Open Source isn‚Äôt dead in AI/ML.\\nAll models released by Google (like BERT, Dreambooth and PaLM), Meta (LLaMa) and Compvision (Stable Diffusion) are open source.\\nThere are several efforts to building open source tooling, that makes it easier to run models, provides auxiliary services like ‚ÄúAPIs for generating embeddings‚Äù and optimizes compute and memory requirements:\\nCog: Is a tool for containerizing ML models from Replicate. Even though replicate is technically a competitor of sorts, they indicate that they have no problem in others using this on their own infrastructure. I‚Äôve used this myself where I used this to containerize and run Stable Diffusion in GCP.\\nChassis: Another product focused on making it easy to containerize ML Models + Data sets\\nNeural Magic‚Äôs technology that enables AI models to run efficiently on CPUs is open source: https://github.com/neuralmagic \\nThere are independent Open Source developers implementing open source versions models almost as fast as the commercial companies do (even for some models from OpenAI): https://github.com/lucidrains?tab=repositories \\nThere are Open Source implementations for auxiliary functions, like ‚ÄúEmbeddings-as-a-Service‚Äù: https://github.com/amansrivastava17/embedding-as-service \\nI‚Äôd love to hear from you if you have references to other open source projects for AI and ML.\\nIt also remains to be seen, whether other internet platforms (particularly open ones, or ones trying to be more open, like twitter), decide to play nice with Open AI or not:\\n \\nAI Workload Types\\nTo help understand the stages in AI model training and use, here is a quick and simplified view. After a base model (like generic BERT or Base Stable Diffusion) has been developed (i.e. the model has been authored, neural network designed, implemented, data tokenized and trained with a base set of parameters), the following are the stages to using it in actual user applications :\\nStep1: Hyperparameter Search or Tuning\\n Goal: Finding the right set of parameters (like ‚Äúlearning rate‚Äù or ‚Äúneural network depth/ layers‚Äù) for the base model you chose and the application you are building. The output is the set of parameters that you will take to step 2\\n Process: Run a small training data set through multiple parameter combinations. Then pick parameters that result in the most accurate output (lowest loss). Can take from a couple hours to a couple days to do this per model, depending on how powerful the GPUs are.\\n Other Considerations: Each of these parameter combinations can be run in parallel and there is no communication necessary between the GPUs running them - i.e ‚ÄúEmbarrassingly Parallel‚Äù problem. Therefore the HPC/ GPU cluster used for hyperparameter search does NOT require high bandwidth networking between the GPU servers.\\n Infrastructure Requirements: High End GPUs preferred (depending on how quickly you want the results and how many epochs/ runs you intend to perform). Availability and Uptime is nice-to-have but not mission critical (because this is not end user facing but just internal to the model/ app builder).\\nStep2: Large-Scale Distributed Training\\n Goal: Training the model + parameters you have identified in Step1 so that it is ready to use in production (in an actual application) for inference (step 3)\\n Process: Typically will divide up the training data set into batches and train them in parallel (on the same model - the one you chose in step 1) so as to make it somewhat of an embarrassingly parallel problem.\\n Other considerations: There will be a lot of communication between the servers running these batches while the distributed training is happening (to feedback outputs of training runs to next run). So it is important to have a high speed communication between the GPU clusters/ providers.\\n Infrastructure Requirements: High End GPUs important to shorten training time. High bandwidth communication between GPU instances IS important. Availability and uptime again are nice to have but not mission critical.\\nStep3: Production Inference\\n Goal: Deploy the model trained in step 2 in production (usually through a user facing product) and have it run predictions (infer things).\\n Process: Deploy the model you trained or pick a model trained by someone else and deploy it. This involves running the model and applying weights necessary to run predictions using the model.\\n Other considerations: In many cases, the users running production inference are not as sophisticated and technical when it comes to AI &amp; ML as those building and training the models. User experience that abstracts infrastructure complexity is important to gain adoption with this user base.\\n Infrastructure Requirements: Can get by with less-than-best GPUs but availability and uptime are critical (since this will be a service used by external users)\\nAI User Ecosystem\\nThe ecosystem of products, companies and services that enable us to train, build and productize AI can be broken down as follows:\\nModel &amp; App Builders\\nThis segment is further divided into 3 sub segments\\nAI Model Creators:\\nThese are scientific groups and companies that have the deepest knowledge and expertise in AI and ML and are engaged in building and training the models that power the applications. Popular models include, Stable Diffusion from CompVis, Google‚Äôs BERT, LaMDA &amp; AudioLM, Open AI‚Äôs Whsiper, GPT and DALL-E, Meta‚Äôs LLaMA, and more. In some cases, these companies and groups also produce the end-user application ‚Äì as in the case of Google BARD and ChatGPT. In other cases the end user app is built by someone else ‚Äì as in the case of Stable Diffusion or ControlNet. These users may be the authors of the model or initial (base) implementors or both. They are also running all 3 steps outlined in the previous section. In most cases, they will have access to their own infrastructure (and in many cases are a big cloud provider itself).\\nAI Model Tuners:\\nThese are users who take a base model and tweak it to improve it in some way to enable it to produce more accurate results. A good example of this is Stable Diffusion, which has two variants, one produced by StabilityAI and another produced by RunwayML. These users often don‚Äôt invent the model themselves but rather take a paper/ proposal and implement it or take an existing implementation and tune it for a specific type of application. These users are NOT inventing the model but usually must run through all 3 stages outlined in the previous section.\\nPure App Developers:\\nThere are a multitude of applications across various Enterprise and Consumer segments that are being built by companies of all sizes (startup to fortune 100). All these applications, rely on a handful of pretrained models (like the ones above) but serve the following categories\\nText Summarization\\nText-to-Speech conversion\\nSpeech-to-Text conversion\\nText Translation\\nDocument Generation\\nCode Generation &amp; Summarization\\nImage Generation &amp; Editing\\nVideo Generation &amp; Editing\\nAudio Generation &amp; Editing\\nInternet Search\\nThese users are typically only running the inference stage of the 3 stages outlined before, because they are taking a pre-trained and tuned model and building a nice user experience on it. They are typically just doing the inference portion of it. In a way, each time you use ChatGPT, you are using one such application that is running inference (aka, making a prediction about what text you are expecting when you ask it a question).\\nAI Tool Builders:\\nThese are teams and companies that are building tools to make it easy to discover, package and deploy AI models and in some cases offer these as services. Examples include, Cog and Chassis (for containerizing ML models), Huggingface (for discovering, sharing, reusing, models and ML datasets - essentially the github for AI/ ML), Replicate and Banana.dev (Deploying+Running AI models) as well as full cycle platforms for training, deploying and managing ML models - like DataRobot, NeptuneAI and VertaAI.\\nAI Infrastructure Providers:\\nThis is very core to all AI and ML development and advancement, regardless of what user segment or application is in question. This includes traditional public cloud providers, newer GPU focused providers (like Lamba Labs and Coreweave) as well as Traditional Datacenter and Cloud Hosting providers starting to offer GPUs.\\n Akash Network (that we are building, with a small core team and large open source community) is unique in this respect, in that it:\\nIs Decentralized (we don‚Äôt own the infrastructure), in fact no one entity owns all of it.\\nIs fully open source (all code on github: https://github.com/akash-network)\\nIs open core and community driven: https://github.com/akash-network/community \\nIs Peer-to-Peer (in theory anyone with a server can become an Akash provider, but most are small or medium, datacenter operators).\\nHas access to a large pool of GPU resources, thanks to availability of GPU power from a large crypto mining community that has relied on Akash for compute for years. Here is an excerpt from an article published by arstechnica last year, that talks about this:\\n \\nSami Kassab (analyst at Messari) has written extensively about this in his latest report.\\n \\nLastly, we will be sharing more about our AI plans at the Cerebral Valley AI Summit and would love to chat with you in-person if you are attending or in the area!\\n \\nIf you are interested in learning more about our offering, please sign up here for early access.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://www.theverge.com/2023/11/22/23973354/a-recent-openai-breakthrough-on-the-path-to-agi-has-caused-a-stir',\n",
       " 'id': 'https://www.theverge.com/2023/11/22/23973354/a-recent-openai-breakthrough-on-the-path-to-agi-has-caused-a-stir',\n",
       " 'title': 'A recent OpenAI breakthrough on the path to AGI has caused a stir.',\n",
       " 'score': 0.19478736817836761,\n",
       " 'published_date': '2023-11-22T00:00:00.000Z',\n",
       " 'author': 'Alex Heath; Richard Lawler',\n",
       " 'image': None,\n",
       " 'subpages': None,\n",
       " 'extras': None,\n",
       " 'text': None,\n",
       " 'highlights': None,\n",
       " 'highlight_scores': None,\n",
       " 'summary': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.results[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A recent OpenAI breakthrough on the path to AGI has caused a stir. Reports from  Reuters  and  The Information  Wednesday night detail an OpenAI model called Q* (pronounced Q Star) that was recently demonstrated internally and is capable of solving simple math problems. Doing grade school math may not seem impressive, but the reports note that, according to the researchers involved, it could be a step toward creating artificial general intelligence (AGI). After the publishing of the Reuters report, which said senior exec Mira Murati told employees that a letter about Q* ‚Äúprecipitated the board‚Äôs actions‚Äù to fire Sam Altman last week, OpenAI spokesperson Lindsey Held Bolton refuted that notion in a statement shared with The Verge: ‚ÄúMira told employees what the media reports were about but she did not comment on the accuracy of the information.‚Äù Separately, a person familiar with the matter told The Verge that the board never received a letter about such a breakthrough and that the company‚Äôs research progress didn‚Äôt play a role in Altman‚Äôs sudden firing. The drama continues!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents.results[0].__dict__['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Result(url='https://www.technologyreview.com/2024/10/23/1105192/ai-hype-index-nov-dec-2024/', id='https://www.technologyreview.com/2024/10/23/1105192/ai-hype-index-nov-dec-2024/', title='Introducing: The AI Hype Index', score=None, published_date='2024-10-23T00:00:00.000Z', author='The Editors', image=None, subpages=None, extras=None, text='Everything you need to know about the state of AI.            There‚Äôs no denying that the AI industry moves fast. Each week brings a bold new announcement, product release, or lofty claim that pushes the bounds of what we previously thought was possible. Separating AI fact from hyped-up fiction isn‚Äôt always easy. That‚Äôs why we‚Äôve created the AI Hype Index‚Äîa simple, at-a-glance summary of everything you need to know about the state of the industry. Our first index is a white-knuckle ride that ranges from the outright depressing‚Äîrising numbers of sexually explicit deepfakes; the complete lack of rules governing Elon Musk‚Äôs Grok AI model‚Äîto the bizarre, including AI-powered dating wingmen and startup Friend‚Äôs dorky intelligent-jewelry line.  But it‚Äôs not all a horror show‚Äîat least not entirely. AI is being used for more wholesome endeavors, too, like simulating the classic video game Doom without a traditional gaming engine. Elsewhere, AI models have gotten so good at table tennis they can now beat beginner-level human opponents. They‚Äôre also giving us essential insight into the secret names monkeys use to communicate with one another. Because while AI may be a lot of things, it‚Äôs never boring.      Deep Dive   Artificial intelligence    Stay connected       Get the latest updates fromMIT Technology Review Discover special offers, top stories,\\nupcoming events, and more.', highlights=None, highlight_scores=None, summary=None),\n",
       " Result(url='https://akhaliq.substack.com/p/trending-ai-news-stories-papers-e68', id='https://akhaliq.substack.com/p/trending-ai-news-stories-papers-e68', title='Trending AI news stories + papers', score=None, published_date='2023-05-15T18:15:20.000Z', author='AK', image='https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f8171c9-69e0-4f9e-a621-dfb00efb7c08_1454x1088.jpeg', subpages=None, extras=None, text=\"Sponsor      Amazon is focusing on using A.I. to get stuff delivered to you faster     Hugging Face releases the first RNN in transformers!      C3.ai stock pops, company raises outlook amid 'accelerating' AI interest     Willow - A Practical, Open Source, Privacy-Focused Platform for Voice Assistants and other Applications     Qualcomm CEO: ‚ÄòA.I. is going to touch every corner of our lives. Here‚Äôs how our devices will change to make it happen‚Äô       MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers abs:  https://arxiv.org/abs/2305.07185     paper page:  https://huggingface.co/papers/2305.07185  abstract: Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.             ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4   abs:  https://arxiv.org/abs/2305.07490     paper page:  https://huggingface.co/papers/2305.07490   model:  https://huggingface.co/Tyrannosaurus/ArtGPT-4  abstract: In recent years, large language models (LLMs) have made significant progress in natural language processing (NLP), with models like ChatGPT and GPT-4 achieving impressive capabilities in various linguistic tasks. However, training models on such a large scale is challenging, and finding datasets that match the model's scale is often difficult. Fine-tuning and training models with fewer parameters using novel methods have emerged as promising approaches to overcome these challenges. One such model is MiniGPT-4, which achieves comparable vision-language understanding to GPT-4 by leveraging novel pre-training models and innovative training strategies. However, the model still faces some challenges in image understanding, particularly in artistic pictures. A novel multimodal model called ArtGPT-4 has been proposed to address these limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100 device in just 2 hours, using only about 200 GB of data. The model can depict images with an artistic flair and generate visual code, including aesthetically pleasing HTML/CSS web pages. Furthermore, the article proposes novel benchmarks for evaluating the performance of vision-language models. In the subsequent evaluation methods, ArtGPT-4 scored more than 1 point higher than the current state-of-the-art model and was only 0.25 points lower than artists on a 6-point scale.             Optimizing Memory Mapping Using Deep Reinforcement Learning   abs:  https://arxiv.org/abs/2305.07440     paper page:  https://huggingface.co/papers/2305.07440  abstract: Resource scheduling and allocation is a critical component of many high impact systems ranging from congestion control to cloud computing. Finding more optimal solutions to these problems often has significant impact on resource and time savings, reducing device wear-and-tear, and even potentially improving carbon emissions. In this paper, we focus on a specific instance of a scheduling problem, namely the memory mapping problem that occurs during compilation of machine learning programs: That is, mapping tensors to different memory layers to optimize execution time. We introduce an approach for solving the memory mapping problem using Reinforcement Learning. RL is a solution paradigm well-suited for sequential decision making problems that are amenable to planning, and combinatorial search spaces with high-dimensional data inputs. We formulate the problem as a single-player game, which we call the mallocGame, such that high-reward trajectories of the game correspond to efficient memory mappings on the target hardware. We also introduce a Reinforcement Learning agent, mallocMuZero, and show that it is capable of playing this game to discover new and improved memory mapping solutions that lead to faster execution times on real ML workloads on ML accelerators. We compare the performance of mallocMuZero to the default solver used by the Accelerated Linear Algebra (XLA) compiler on a benchmark of realistic ML workloads. In addition, we show that mallocMuZero is capable of improving the execution time of the recently published AlphaTensor matrix multiplication model.             Universal Source Separation with Weakly Labelled Data   abs:  https://arxiv.org/abs/2305.07447     paper page:  https://huggingface.co/papers/2305.07447   github:  https://github.com/bytedance/uss  abstract: Universal source separation (USS) is a fundamental research task for computational auditory scene analysis, which aims to separate mono recordings into individual source tracks. There are three potential challenges awaiting the solution to the audio source separation task. First, previous audio source separation systems mainly focus on separating one or a limited number of specific sources. There is a lack of research on building a unified system that can separate arbitrary sources via a single model. Second, most previous systems require clean source data to train a separator, while clean source data are scarce. Third, there is a lack of USS system that can automatically detect and separate active sound classes in a hierarchical level. To use large-scale weakly labeled/unlabeled audio data for audio source separation, we propose a universal audio source separation framework containing: 1) an audio tagging model trained on weakly labeled data as a query net; and 2) a conditional source separation model that takes query net outputs as conditions to separate arbitrary sound sources. We investigate various query nets, source separation models, and training strategies and propose a hierarchical USS strategy to automatically detect and separate sound classes from the AudioSet ontology. By solely leveraging the weakly labelled AudioSet, our USS system is successful in separating a wide variety of sound classes, including sound event separation, music source separation, and speech enhancement. The USS system achieves an average signal-to-distortion ratio improvement (SDRi) of 5.57 dB over 527 sound classes of AudioSet; 10.57 dB on the DCASE 2018 Task 2 dataset; 8.12 dB on the MUSDB18 dataset; an SDRi of 7.28 dB on the Slakh2100 dataset; and an SSNR of 9.00 dB on the voicebank-demand dataset.             Better speech synthesis through scaling   abs:  https://arxiv.org/abs/2305.07243     paper page:  https://huggingface.co/papers/2305.07243  abstract: In recent years, the field of image generation has been revolutionized by the application of autoregressive transformers and DDPMs. These approaches model the process of image generation as a step-wise probabilistic processes and leverage large amounts of compute and data to learn the image distribution. This methodology of improving performance need not be confined to images. This paper describes a way to apply advances in the image generative domain to speech synthesis. The result is TorToise -- an expressive, multi-voice text-to-speech system.            Thank you for reading AK‚Äôs Substack. This post is public so feel free to share it.   Share\", highlights=None, highlight_scores=None, summary=None),\n",
       " Result(url='https://radicaldatascience.wordpress.com/2024/09/16/ai-news-briefs-bulletin-board-for-september-2024/', id='https://radicaldatascience.wordpress.com/2024/09/16/ai-news-briefs-bulletin-board-for-september-2024/', title='AI News Briefs BULLETIN BOARD for September\\xa02024', score=None, published_date='2024-09-16T00:00:00.000Z', author='', image=None, subpages=None, extras=None, text='Welcome to the AI News Briefs Bulletin Board, a timely new channel bringing you the latest industry insights and perspectives surrounding the field of AI including deep learning, large language models, generative AI, and transformers. I am working tirelessly to dig up the most timely and curious tidbits underlying the day‚Äôs most popular technologies. I know this field is advancing rapidly and I want to bring you a regular resource to keep you informed and state-of-the-art. The news bites are constantly being added in reverse date order (most recent on top). With the bulletin board you can check back often to see what‚Äôs happening in our rapidly accelerating industry. Click HERE to check out previous ‚ÄúAI News Briefs‚Äù round-ups.\\n    \\n    \\n[9/17/2024] Google Launched NotebookLM with AI-Powered Audio Podcast and Summaries ‚Äì NotebookLM is Google Labs‚Äô new tool that converts uploaded documents into AI-generated audio discussions in podcast format. It transforms text from PDFs, Google Docs, and Slides into podcast-style conversations between two AI hosts. This approach aims to enhance information retention through auditory learning.\\nThe system uses Gemini 1.5‚Äôs multimodal capabilities to process various input formats. It analyzes content, extracts key information, and generates responses grounded in the source material. The system employs advanced text-to-speech synthesis for AI host voices. It generates contextually relevant dialogue, maintaining coherence with source material. However, occasional inaccuracies may occur, and user interaction during playback is not yet supported. NotebookLM provides citations and relevant quotes to support its outputs. Audio Overview‚Äôs core innovation lies in its AI-driven dialogue synthesis. The system creates a conversational summary of uploaded content, with AI agents discussing main points and connecting topics. \\nReported technical specifications:\\nContext window capacity: ~50 PDFs\\nLanguage support: Currently English-only\\nInput formats: PDFs, Google Docs, Slides, web URLs\\nOutput: Downloadable audio file\\nNotebookLM‚Äôs architecture manages large input volumes and cross-references information between multiple documents. \\nHowever, audio generation for extensive notebooks can take several minutes, indicating computational intensity.\\n    \\n[9/17/2024] NEW research paper: ‚ÄúLLMs Will Always Hallucinate, and We Need to Live With This.‚Äù ‚Äì LLMs inevitably hallucinate due to fundamental mathematical and logical limitations. The paper proves hallucinations are structural and cannot be eliminated through architectural improvements, dataset enhancements, or fact-checking mechanisms.\\nThe authors introduce the concept of ‚ÄúStructural Hallucinations‚Äù and provide mathematical proofs using computational theory and G√∂del‚Äôs First Incompleteness Theorem. They demonstrate undecidability in key LLM processes: training data completeness, information retrieval, intent classification, output generation, and fact-checking.\\nThe paper proves that every stage of LLM processing has a non-zero probability of producing hallucinations. It illustrates these concepts using popular LLMs (OpenAI, Gemini, Claude) on a specific prompt, showing deviation from expected responses.\\n    \\n[9/17/2024] NEW research paper: ‚ÄúWhat is the Role of Small Models in the LLM Era: A Survey,‚Äù ‚Äì This paper explores the role of small models in the era of LLMs. It addresses the challenges of high computational costs and energy consumption associated with large models, which limit their accessibility for many researchers and businesses. The paper investigates how small models can complement or compete with LLMs in various scenarios.\\nThe authors systematically examine the relationship between LLMs and small models from two perspectives: collaboration and competition. They analyze how small models can enhance LLMs in tasks like data curation, efficient inference, and deficiency repair. Conversely, they explore how LLMs can improve small models through knowledge distillation and data synthesis.\\nThe research reveals that small models remain highly popular, with BERT-base being one of the most downloaded models. It identifies scenarios where small models are preferable, such as computation-constrained environments, task-specific applications, and situations requiring interpretability. The paper provides valuable insights for practitioners on optimizing resource usage and developing cost-effective systems.\\n    \\n    \\n[9/17/2024] CalypsoAI just launched a new, cybercrime-solving game called ‚ÄúBehind the Mask.‚Äù The desktop game showcases the power of CalypsoAI‚Äôs GenAI scanners and gives consumers the unique opportunity to interact with CalypsoAI‚Äôs technology and GenAI in a fun and compelling way.\\nThe premise of the game is that YOU are the investigator, trying to identify all the five hackers stealing valuable cryptocurrency‚ÄîCalyptoCoins. Good news: you have the help of an AI informant. You must ask it the right questions to identify and stop the theft before more CalyptoCoins are lost. But there‚Äôs a twist‚Ä¶ The hackers are using CalypsoAI‚Äôs technology to protect that information, and the GenAI scanners learn from each ‚Äúleak‚Äù and improve defenses‚Äîmaking each level harder than the last. So, can you outsmart AI?\\n    \\n[9/16/2024] AI is accelerating the climate crisis ‚Äì If you care about the environment, think twice about using AI. Generative artificial intelligence uses 30 times more energy than a traditional search engine, warns researcher Sasha Luccioni, on a mission to raise awareness about the environmental impact of the hot new technology.\\n    \\n    \\n[9/16/2024] At its Dreamforce conference in San Francisco this week, Salesforce Ventures announced an additional $500 million fund dedicated to AI companies, bringing its total AI investment fund to $1 billion. This marks a significant increase, as Salesforce Ventures doubled its AI fund to $500 million in June 2023.\\nSince its launch in 2009, Salesforce Ventures has deployed $5 billion, and its growing AI portfolio includes notable companies like Anthropic, Hugging Face, Runway, and Together AI. San Francisco‚Äôs AI boom is attracting startups from around the world, with Salesforce Ventures playing a key role in fostering the city‚Äôs AI ecosystem.\\n    \\n[9/16/2024] Jeff Dean, on the long term potential of multi-modal models like Gemini ‚Äì Learn about Google‚Äôs AI journey from Jeff Dean. Explore the evolution of neural networks, Google Brain, and DeepMind. Discover insights on large-scale model training, reinforcement learning, and multimodal AI. Understand the development of Transformers and Gemini. Gain knowledge on cutting-edge AI techniques and their practical applications.\\n \\n \\n \\n    \\n[9/16/2024] Tweet by Astrophysics Ph.D. goes viral demoing Open AI Strawberry model replicated 10 months of Ph.D. coding work in ~ 6 prompts using paper‚Äôs methods section ‚Äì Dr. Kyle Kabasares found that after about 6 prompts, ChatGPT o1‚Äôs preview and mini created a running version of the code described from the methods section of his research paper. He emphasizes that while the skeletal code does emulate what his actual code does, it did use its own synthetic data that he asked for it to create as opposed to real astronomical data that would be used in a real paper. Nevertheless, the potential it has is incredible to effectively accomplish what he struggled for about 10 months in the first year of his Ph.D. at UC Irvine (measuring black hole masses by modeling astronomical data).', highlights=None, highlight_scores=None, summary=None)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_contents.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
